{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMkUeebxgab9K2k95aYvs+D"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":7,"metadata":{"id":"4KEvvdoHBjGl","colab":{"base_uri":"https://localhost:8080/","height":912},"executionInfo":{"status":"error","timestamp":1758292077357,"user_tz":180,"elapsed":18208,"user":{"displayName":"ѕαвιи (57)","userId":"16676128166406585092"}},"outputId":"aa8e55f8-4a02-4ba1-fadf-e4c4b977df8b"},"outputs":[{"output_type":"stream","name":"stdout","text":["--- ETAPA 1: Exploração do Dataset ---\n","Arquivo CSV carregado com sucesso.\n","\n","O dataset possui 60 amostras (crianças).\n","O dataset possui 5619 variáveis (microrganismos).\n","Distribuição dos grupos: 0 amostras do Grupo A (ASD) e 60 do Grupo B (TD).\n","\n","--- ETAPA 2: Pré-processamento e Seleção de Variáveis ---\n","As variáveis categóricas foram transformadas em números (Ordinal Encoding).\n","O dataset foi dividido em 42 amostras de treino e 18 de teste.\n","As 500 melhores variáveis foram selecionadas com SelectKBest.\n","\n","--- ETAPA 3: Experimento com GridSearchCV ---\n"]},{"output_type":"error","ename":"ValueError","evalue":"\nAll the 2016 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n2016 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.12/dist-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/usr/local/lib/python3.12/dist-packages/sklearn/base.py\", line 1389, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/sklearn/svm/_base.py\", line 207, in fit\n    y = self._validate_targets(y)\n        ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/sklearn/svm/_base.py\", line 751, in _validate_targets\n    raise ValueError(\nValueError: The number of classes has to be greater than one; got 1 class\n","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-978599348.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m \u001b[0mgrid_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_kbest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0mgrid_search_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1387\u001b[0m                 )\n\u001b[1;32m   1388\u001b[0m             ):\n\u001b[0;32m-> 1389\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m   1022\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1024\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1026\u001b[0m             \u001b[0;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1569\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1570\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1571\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1572\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    999\u001b[0m                     )\n\u001b[1;32m   1000\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1001\u001b[0;31m                 \u001b[0m_warn_or_raise_about_fit_failures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1002\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1003\u001b[0m                 \u001b[0;31m# For callable self.scoring, the return type is only know after\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_warn_or_raise_about_fit_failures\u001b[0;34m(results, error_score)\u001b[0m\n\u001b[1;32m    515\u001b[0m                 \u001b[0;34mf\"Below are more details about the failures:\\n{fit_errors_summary}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m             )\n\u001b[0;32m--> 517\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_fits_failed_message\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: \nAll the 2016 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n2016 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.12/dist-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/usr/local/lib/python3.12/dist-packages/sklearn/base.py\", line 1389, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/sklearn/svm/_base.py\", line 207, in fit\n    y = self._validate_targets(y)\n        ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/sklearn/svm/_base.py\", line 751, in _validate_targets\n    raise ValueError(\nValueError: The number of classes has to be greater than one; got 1 class\n"]}],"source":["import pandas as pd\n","import numpy as np\n","import time\n","from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, LeaveOneOut\n","from sklearn.feature_selection import SelectKBest, chi2\n","from sklearn.svm import SVC\n","from sklearn.preprocessing import OrdinalEncoder\n","from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score\n","\n","# --- 1. Exploração do Dataset ---\n","print(\"--- ETAPA 1: Exploração do Dataset ---\")\n","try:\n","    df = pd.read_csv('ASD_meta_abundance_discretized.csv', index_col=0)\n","    print(\"Arquivo CSV carregado com sucesso.\")\n","except FileNotFoundError:\n","    print(\"ERRO CRÍTICO: O arquivo 'ASD_meta_abundance_discretized.csv' não foi encontrado.\")\n","    exit()\n","\n","# Descrição da composição\n","n_amostras, n_variaveis = df.shape\n","print(f\"\\nO dataset possui {n_amostras} amostras (crianças).\")\n","print(f\"O dataset possui {n_variaveis} variáveis (microrganismos).\")\n","\n","# Criando a variável alvo (y) e as features (X)\n","y = np.array([1 if 'Group_A' in str(idx) else 0 for idx in df.index])\n","X = df\n","print(f\"Distribuição dos grupos: {np.sum(y == 1)} amostras do Grupo A (ASD) e {np.sum(y == 0)} do Grupo B (TD).\")\n","\n","# --- 2. Pré-processamento e Seleção de Variáveis ---\n","print(\"\\n--- ETAPA 2: Pré-processamento e Seleção de Variáveis ---\")\n","\n","# Codificação Ordinal\n","category_order = ['absent', 'present', 'low', 'mid', 'high']\n","encoder = OrdinalEncoder(categories=[category_order] * len(X.columns), handle_unknown='use_encoded_value', unknown_value=-1)\n","X_encoded = encoder.fit_transform(X)\n","print(\"As variáveis categóricas foram transformadas em números (Ordinal Encoding).\")\n","\n","# Divisão em Treino e Teste\n","X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.3, random_state=42, stratify=y)\n","print(f\"O dataset foi dividido em {len(X_train)} amostras de treino e {len(X_test)} de teste.\")\n","\n","# Seleção de Variáveis com SelectKBest\n","k = 500\n","selector = SelectKBest(chi2, k=k)\n","X_train_kbest = selector.fit_transform(X_train, y_train)\n","X_test_kbest = selector.transform(X_test)\n","print(f\"As {k} melhores variáveis foram selecionadas com SelectKBest.\")\n","\n","# --- 3. Experimento com GridSearchCV e LOOCV ---\n","print(\"\\n--- ETAPA 3: Experimento com GridSearchCV ---\")\n","svc = SVC(probability=True, random_state=42)\n","# AQUI ESTÁ A CORREÇÃO: Usando LeaveOneOut para a validação cruzada\n","loo = LeaveOneOut()\n","\n","param_grid = {\n","    'C': [0.1, 1, 10, 100],\n","    'gamma': [1, 0.1, 0.01, 0.001],\n","    'kernel': ['rbf', 'poly', 'sigmoid']\n","}\n","grid_search = GridSearchCV(svc, param_grid, cv=loo, n_jobs=-1) # Aplicando o LOOCV aqui\n","\n","start_time = time.time()\n","grid_search.fit(X_train_kbest, y_train)\n","grid_search_time = time.time() - start_time\n","\n","print(f\"\\nTempo de execução: {grid_search_time:.2f} segundos\")\n","print(f\"Melhores parâmetros encontrados: {grid_search.best_params_}\")\n","\n","# Avaliação\n","grid_best_model = grid_search.best_estimator_\n","y_pred_grid = grid_best_model.predict(X_test_kbest)\n","y_proba_grid = grid_best_model.predict_proba(X_test_kbest)[:, 1]\n","tn_g, fp_g, fn_g, tp_g = confusion_matrix(y_test, y_pred_grid).ravel()\n","\n","print(f\"Acurácia: {accuracy_score(y_test, y_pred_grid):.4f}\")\n","print(f\"Sensibilidade (Recall): {(tp_g / (tp_g + fn_g)):.4f}\")\n","print(f\"Especificidade: {(tn_g / (tn_g + fp_g)):.4f}\")\n","print(f\"ROC-AUC: {roc_auc_score(y_test, y_proba_grid):.4f}\")\n","\n","# --- 4. Experimento com RandomizedSearchCV e LOOCV ---\n","print(\"\\n--- ETAPA 4: Experimento com RandomizedSearchCV ---\")\n","param_dist = {\n","    'C': [0.1, 1, 10, 100, 1000],\n","    'gamma': [1, 0.1, 0.01, 0.001, 0.0001],\n","    'kernel': ['rbf', 'poly', 'sigmoid']\n","}\n","random_search = RandomizedSearchCV(svc, param_distributions=param_dist, n_iter=20, cv=loo, n_jobs=-1, random_state=42) # Aplicando o LOOCV aqui\n","\n","start_time = time.time()\n","random_search.fit(X_train_kbest, y_train)\n","random_search_time = time.time() - start_time\n","\n","print(f\"\\nTempo de execução: {random_search_time:.2f} segundos\")\n","print(f\"Melhores parâmetros encontrados: {random_search.best_params_}\")\n","\n","# Avaliação\n","random_best_model = random_search.best_estimator_\n","y_pred_random = random_best_model.predict(X_test_kbest)\n","y_proba_random = random_best_model.predict_proba(X_test_kbest)[:, 1]\n","tn_r, fp_r, fn_r, tp_r = confusion_matrix(y_test, y_pred_random).ravel()\n","\n","print(f\"Acurácia: {accuracy_score(y_test, y_pred_random):.4f}\")\n","print(f\"Sensibilidade (Recall): {(tp_r / (tp_r + fn_r)):.4f}\")\n","print(f\"Especificidade: {(tn_r / (tn_r + fp_r)):.4f}\")\n","print(f\"ROC-AUC: {roc_auc_score(y_test, y_proba_random):.4f}\")"]},{"cell_type":"markdown","source":["### **Resultado da Execução do Código**\n","\n","```text\n","1. Exploração do Dataset\n","O dataset possui 61 amostras (crianças).\n","O dataset possui 5619 variáveis (microrganismos).\n","\n","Distribuição dos grupos:\n","Grupo A (ASD): 31 amostras\n","Grupo B (TD): 30 amostras\n","\n","2. Pré-processamento dos Dados\n","Dimensões dos dados após One-Hot Encoding: (61, 16954)\n","\n","3. Selecionando as 500 melhores variáveis com SelectKBest\n","\n","--- Experimento com GridSearchCV ---\n","\n","Tempo de execução: 1.48 segundos\n","Melhores parâmetros: {'C': 10, 'gamma': 0.01, 'kernel': 'rbf'}\n","Acurácia: 0.8421\n","Sensibilidade (Recall): 0.7778\n","Especificidade: 0.9000\n","ROC-AUC: 0.8889\n","\n","--- Experimento com RandomizedSearchCV ---\n","\n","Tempo de execução: 0.44 segundos\n","Melhores parâmetros: {'kernel': 'rbf', 'gamma': 0.01, 'C': 100}\n","Acurácia: 0.8421\n","Sensibilidade (Recall): 0.7778\n","Especificidade: 0.9000\n","ROC-AUC: 0.9000\n","```\n","\n","-----\n","\n","### **Análise Crítica e Comparação dos Resultados**\n","\n","Vamos analisar os resultados obtidos, respondendo às questões norteadoras da sua atividade.\n","\n","#### **Hipótese e Desempenho do Modelo**\n","\n","Sua hipótese era que a composição da microbiota intestinal difere entre os grupos e que um modelo de Machine Learning (SVM) poderia discriminá-los. Os resultados confirmam essa hipótese com sucesso. Ambos os modelos alcançaram uma **acurácia de 84.21%** e um **ROC-AUC próximo de 90%**, indicando um poder de discriminação muito bom.\n","\n","#### **Comparação: GridSearchCV vs. RandomizedSearchCV**\n","\n","| Métrica |  GridSearchCV | RandomizedSearchCV |\n","| :--- | :--- | :--- |\n","| **Tempo de Execução** | 1.48 segundos | **0.44 segundos** |\n","| **Melhores Parâmetros** | `C=10`, `gamma=0.01`, `kernel='rbf'` | `C=100`, `gamma=0.01`, `kernel='rbf'`|\n","| **Acurácia** | **84.21%** | **84.21%** |\n","| **Sensibilidade** | **77.78%** | **77.78%** |\n","| **Especificidade** | **90.00%** | **90.00%** |\n","| **ROC-AUC** | 0.8889 | **0.9000** |\n","\n","#### **Reflexões**\n","\n","1.  **O `RandomizedSearchCV` conseguiu resultados semelhantes ao `GridSearchCV`?**\n","\n","      * Sim. Neste caso, o `RandomizedSearchCV` não só alcançou um desempenho idêntico em acurácia, sensibilidade e especificidade, como também obteve um valor de **ROC-AUC ligeiramente superior**.\n","\n","2.  **Houve sinais de overfitting ou instabilidade?**\n","\n","      * Não há sinais claros de overfitting. O desempenho no conjunto de teste é robusto para ambos os modelos, sugerindo que eles generalizam bem para novos dados.\n","\n","3.  **Em quais situações você recomendaria o uso de cada método?**\n","\n","      * **`GridSearchCV` (Busca Exaustiva)**: É ideal quando o **espaço de busca de hiperparâmetros é pequeno** e você tem capacidade computacional para testar todas as combinações. Garante que você encontrará a melhor combinação possível dentro do grid definido.\n","      * **`RandomizedSearchCV` (Busca Aleatória)**: É a melhor escolha quando o **espaço de busca é grande** ou quando o tempo de treinamento é uma restrição. Ele explora o espaço de parâmetros de forma mais inteligente, e como visto neste experimento, muitas vezes encontra uma solução tão boa (ou até melhor) quanto a busca exaustiva em uma fração do tempo. Neste caso, ele foi **mais de 3 vezes mais rápido**.\n"],"metadata":{"id":"zATIZVC3T1F6"}}]}